# AI音声モデル「エイダ」開発プロジェクト

このリポジトリは、Coqui-TTSを利用したAI音声モデル「エイダ」のローカル開発環境です。開発の全工程（環境設定、データ準備、学習、音声合成）は、Web UI**「コントロールパネル」**を通じて、直感的かつ一元的に管理できます。

![コントロールパネルのスクリーンショット](https://example.com/control_panel_screenshot.png) 
*(ここにコントロールパネルのスクリーンショット画像を配置すると、より分かりやすくなります)*

## 🚀 使い方 (クイックスタート)

#### 1. リポジトリのクローン
```bash
git clone https://github.com/LOU-Ark/ADA-Voice-Project.git
cd ADA-Voice-Project
```

#### 2. バックエンドサーバーの起動
このターミナルは、作業が完了するまで常に開いておいてください。
```bash
# 必要なライブラリをインストール
pip install Flask Flask-Cors Flask-SocketIO python-engineio simple-websocket

# サーバーを起動
python server.py
```
サーバーが起動すると、`* Running on http://127.0.0.1:5000` のようなメッセージが表示されます。

#### 3. コントロールパネルを開く
Webブラウザで、このリポジトリのルートにある `control_panel.html` ファイルを直接開きます。
> **注意:** サーバーが起動している状態でファイルを開いてください。

#### 4. UIから実行
ブラウザに表示されたステップに従い、ボタンをクリックしてプロジェクトを進めてください。

## 🛠️ コントロールパネル (UI) の解説

`control_panel.html` は、このプロジェクトを操作するためのメインインターフェースです。

### 操作パネル

画面左側には、プロジェクトを進行させるための4つのステップがあります。完了済みのステップは自動でスキップされ、次に実行すべきステップがアクティブになります。

1.  **環境設定:** Python仮想環境 (`tts_env`) を構築し、学習に必要なライブラリをインストールします。
2.  **データ準備:** `TTS/my_ada_voice/wavs_original/` に配置された音声データを処理し、学習用の`metadata.csv`を生成します。
3.  **学習:** モデルの学習を開始します。学習を停止するためのボタンもここにあります。
4.  **音声合成:** 学習済みの最新モデルを使い、テキスト入力から音声を合成・再生します。このステップは、有効なモデルファイルが存在する場合にのみ利用可能です。

### リアルタイムモニター

画面右側には、学習の進捗をリアルタイムで確認するためのダッシュボードとログが表示されます。

-   **ステータス表示:** 現在のEPOCH、GLOBAL STEP、Gitのプッシュ状況などが表示されます。
-   **学習グラフ:** 学習が進むと、ここに損失（Loss）の推移が折れ線グラフで表示されます。グラフが右肩下がりになっていれば、学習がうまくいっている証拠です。
-   **リアルタイムログ:** バックエンドで実行されているコマンドの出力が、ここにリアルタイムで表示されます。

## ⚙️ 内部の仕組み (技術解説)

このコントロールパネルは、以下の要素が連携して動作しています。

-   **フロントエンド (`control_panel.html`)**:
    ユーザーからのボタンクリックなどの操作を受け付けます。受け付けた命令は、Socket.IOという技術を使って、ローカルで動いているバックエンドサーバーに送信されます。

-   **バックエンド (`server.py`)**:
    Flaskという軽量なWebフレームワークで構築されたPythonサーバーです。フロントエンドからの命令（例：「学習を開始して」）を受け取ると、対応するシェルスクリプトやPythonスクリプトをサブプロセスとしてPC上で実行します。実行中のログは、Socket.IOを通じてリアルタイムでフロントエンドに送り返されます。

-   **データ準備 (`rebuild_metadata.py`)**:
    「データ準備」ボタンが押された際に実行されるPythonスクリプトです。`wavs_original`内の音声ファイルをサニタイズ（モノラル化など）し、ファイル名をリネームして`wavs`フォルダに保存します。同時に、元のファイル名から抽出・整形したテキストと新しいファイル名を紐付けた`metadata.csv`を生成します。

-   **学習と自動プッシュ (`server.py`内のロジック)**:
    「学習開始」ボタンが押されると、`server.py`は`train_tts.py`を実行します。サーバーは学習ログを一行ずつ監視し、`> BEST MODEL`という文字列を検知すると、Git LFSで管理されている最新のモデルファイルを自動で`git add`, `commit`, `push`します。

## 主要な使用技術

-   **AI / 音声合成**: Coqui-TTS
-   **バックエンド**: Python, Flask, Flask-SocketIO
-   **フロントエンド**: HTML, JavaScript, Tailwind CSS, Chart.js
-   **バージョン管理**: Git, Git LFS

---

### READMEの配置について

この新しい`README.md`は、**リポジトリのルートディレクトリ**に配置するのが最適です。
一方、これまでのWebレポートに関するREADMEは、`docs`フォルダ内に移動させ、`docs/README.md` として保存すると、情報の棲み分けができて良いでしょう。

**推奨するファイル構成:**
```
ADA-Voice-Project/
├── README.md             <-- 今回作成した、開発者向けREADME
├── control_panel.html
├── server.py
├── rebuild_metadata.py
├── ... (その他の開発ファイル)
└── docs/
    ├── README.md         <-- これまでのWebレポートに関するREADME
    ├── index.html
    ├── style.css
    └── script.js
```